---
title: "Simulated Response Modeling"
author: "Miller, J."
date: "April 4, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = 'C:/users/jmiller/Desktop/response_modeling/data')
```

## Overview

Analysis and modeling of customer, citizen, or patient response to outreach is a ubiquitous use case for Data Science (DS), with elevated prevalence and return-on-investment (ROI) in the areas of advertising and marketing.

This presentation leverages public data and open source technologies to demonstrate a curated subset of Data Science strategies for such response analytic and modeling.

Public data was taken from [a Kaggle competition](https://www.kaggle.com/c/springleaf-marketing-response) (circa 2016) sponsored by Springleaf - a lending organization now known as [OneMain Financial](https://www.onemainfinancial.com/ppc/v7).


## Springleaf Competition Details

A few notes about the original competition this data comes from:

 * Took place in 2016 
 * 2,226 Data Science teams participated (approx. 5,000-7,000 total participants)
 * $100,000 USD prize 
 * Winning team ("Asian Ensemble") had 6 members from Stanford University
 * About 200 teams' models performed within 1% as good as 1st place
 * Jason's former instructor ranked 6th

![Finalized Private Leaderboard from Springleaf](../images/private_leaderboard.png)
 
Although Jason was on Kaggle at the time, Pareto's law dictated that he allocate the after-hours DS to StackOverflow and Coursera instead of Springleaf.  


## Springleaf Data Description (official)

You are provided a high-dimensional data set of anonymized customer information. Each row corresponds to one customer. The response variable is binary and labeled "target". You must predict the target variable for every row in the test set.

The features have been anonymized to protect privacy and are comprised of a mix of continuous and categorical features. You will encounter many "placeholder" values in the data, which represent cases such as missing values. We have intentionally preserved their encoding to match with internal systems at Springleaf. The meaning of the features, their values, and their types are provided "as-is" for this competition; handling a huge number of messy features is part of the challenge here.


## Springleaf Evaluation Metric

For this competition results were evaluated on [area under the ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) (AUC) curve between the predicted probability and the observed target.

The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The true-positive rate is also known as sensitivity, recall or probability of detection[4] in machine learning. The false-positive rate is also known as the fall-out or probability of false alarm[4] and can be calculated as (1 âˆ’ specificity).  


## Additional Data Notes (from Jason)

 * The data set physical memory size was approximately 2GB (train and test combined)
 * Training and Testing data sets each had about 145,000 rows of observations
 * Training and Testing data sets each had about 2,000 columns of raw features (slightly fewer)
 

## Interview vs. Competition

For the purposes of this interview I'd like to both show a competition-suitable response model and demonstrate a few additional industry-relavent concepts.  


## Methodology

A multi-phase ensemble strategy employed to validate and compare several specifications and base learner algorithms.

Repeated K-fold CV was performed in-sample. Some algorithms further employed bagging and boosting as training-time CV.

A full 50% of the data was held out for final validation.


## EDA

Given extremely limited time to work on this presentation, EDA was kept minimal. In major real-world projects, I typically take the opposite approach. In this case there was also a wealth of existing EDA.

Let's review a few useful data visualizations that came from EDA.

A Pearson correlation matrix using heatmap coloring, performed on 100 randomly sampled features:

![](../images/corr.png)

The Spearman equivalent:

![Spearman Correlation Matrix](../images/spearman.png)


Plotting of apparent geographic features on a map space:

![Anonymized Geographic Features](../images/maps.png)

As both EDA and feature generation, I measured counts of how many times 1 feature was greater than another:

![Feature Generation](../images/how_many_times_greater.png)



## Modeling

This morning I trained the following models:

 * Gradient Boosting
 * Random Forest
 * Logit
 * Neural Network
 * eXtreme Gradient Boosting
 
Base learners were input to a stacked ensemble (superlearner).

### XGB 
```{r}
# ##########################################
# # File:            xgb.R                 #
# # Context:         Run by Master file    #
# # Contact author:  Miller, J.            #
# # Initial Date:    2019-04-04            #
# # Version:         0.0.1                 #
# ##########################################
# 
# setdiff2(colnames(train),
#          colnames(test))
# 
# cat("training a XGBoost classifier\n")
# clf <- xgboost(data        = data.matrix(train[,!colnames(train) %in% c("Y", "target", "ID")]),
#                label       = train$Y,
#                nrounds     = 20,
#                objective   = "binary:logistic",
#                eval_metric = "auc")
# saveRDS(clf, "clf.RDS")
# 
# cat("making predictions in batches due to 8GB memory limitation\n")
# submission <- data.frame(ID=test$ID)
# submission$target <- NA 
# 
# # predict in chunks
# for (rows in split(1:nrow(test), ceiling((1:nrow(test))/10000))) {
#   submission[rows, "target"] <- predict(clf, data.matrix(test[rows, colnames(test) %in% clf$feature_names]))
# }
# 
# 
# cat("saving the output file... \n")
# write_csv(submission, "xgboost_submission.csv")

```


### H2O

```{r}
# ##########################################
# # File:            ml1.R                 #
# # Context:         Run by Master file    #
# # Contact author:  Miller, J.            #
# # Initial Date:    2019-04-04            #
# # Version:         0.0.1                 #
# # Description:     NN Metalearning       #
# ##########################################
# 
# 
# 
# h2o.init()
# 
# train <- readRDS("train_checkpoint1.RDS")
# test  <- readRDS("test_checkpoint1.RDS")
# 
# # Number of folds for CV
# nfolds <- 5  
# 
# # Identify predictors and response
# y <- "Y"
# x <- setdiff(names(train), y)
# 
# # For binary classification, response should be a factor
# train.h2o <- as.h2o(train[1:(nrow(train)-5000),])
# test.h2o  <- as.h2o(train[(nrow(train)-5000+1):nrow(train),])
# 
# train.h2o[,y] <- as.factor(train.h2o$Y)
# test.h2o[,y]  <- as.factor(test.h2o$Y)
# 
# 
# # Base Learning
# glm1 <- h2o.glm(x = x, y = y, family = "binomial", 
#                 training_frame = train.h2o,
#                 nfolds = nfolds,
#                 fold_assignment = "Modulo",
#                 keep_cross_validation_predictions = TRUE)
# h2o.saveModel(glm1, "h2o_glm1")
# # [1] "C:\\Users\\jmiller\\Desktop\\response_modeling\\data\\h2o_glm1\\GLM_model_R_1554476314148_1"
# 
# gbm1 <- h2o.gbm(x = x, y = y, distribution = "bernoulli",
#                 training_frame = train.h2o,
#                 seed = 1,
#                 nfolds = nfolds,
#                 fold_assignment = "Modulo",
#                 keep_cross_validation_predictions = TRUE)
# 
# rf1 <- h2o.randomForest(x = x, y = y, # distribution not used for RF
#                         training_frame = train.h2o,
#                         seed = 1,
#                         nfolds = nfolds,
#                         fold_assignment = "Modulo",
#                         keep_cross_validation_predictions = TRUE)
# 
# dl1 <- h2o.deeplearning(x = x, y = y, distribution = "bernoulli",
#                         training_frame = train.h2o,
#                         nfolds = nfolds,
#                         fold_assignment = "Modulo",
#                         keep_cross_validation_predictions = TRUE)
# 
# models <- list(glm1, gbm1, rf1, dl1)
# metalearner <- "h2o.glm.wrapper"
# 
# ensemble <- h2o.stackedEnsemble(x = x,
#                                 y = y,
#                                 training_frame = train.h2o,
#                                 model_id = "my_ensemble_binomial",
#                                 base_models = list(dl1, rf1, gbm1))
# 
# 
# # Compute test set performance on OOF holdout:
# perf <- h2o.performance(ensemble, newdata = test.h2o)
# print(perf)
# 
# h2o.saveModel(rf1,"h2o_rf1")
# h2o.saveModel(dl1,"h2o_dl1")
# h2o.saveModel(ensemble, "h2o_stack")
# 
#h2o.shutdown()
```
 

## Validation

Repeated k-fold validation was performed in-sample during training time. 50% of the final data was saved as a holdout for final validation.

Our final validation AUC was

        0.80011

Which would have earned a rank of 11th place, however I benefited from knowledge and libraries only available after the competition.

```{r}
# H2OBinomialMetrics: stackedensemble
# 
# MSE:  0.1039833
# RMSE:  0.3224644
# LogLoss:  0.3752801
# Mean Per-Class Error:  0.2087087
# AUC:  0.80011
# 
# Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
#           0    1    Error        Rate
# 0      2728 1087 0.284928  =1087/3815
# 1       294  891 0.2481013   =294/1185
# Totals 3022 1978 0.276200  =1381/5000

```


## Next Steps

There were a lot of things I'd do in real life, which time did not allow for.

These include:

  * Advanced EDA
  * Target Encoding
  * More baselaerners
  * 3rd-stage ensemble combining xgboost, caret, and h2o
  
  